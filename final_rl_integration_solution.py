#!/usr/bin/env python3
"""
Final RL Integration Solution
Complete solution for integrating RL with your Master AI system
"""

import os
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
import logging

# Import your existing Master AI Controller
from master_ai_controller import MasterAIController

logger = logging.getLogger(__name__)

class FinalRLMasterAI(MasterAIController):
    """
    Final Master AI Controller with integrated RL capabilities
    """
    
    def __init__(self, config: Dict = None):
        super().__init__(config)
        
        # RL enhancement system
        self.rl_system = {
            'enabled': False,
            'model_loaded': False,
            'model_path': None,
            'performance_tracker': {
                'total_decisions': 0,
                'action_history': [],
                'performance_scores': [],
                'learning_episodes': 0
            },
            'config': {
                'exploration_rate': 0.1,
                'learning_rate': 0.001,
                'state_size': 20,
                'action_size': 8,
                'reward_weights': {
                    'queue_reduction': 1.0,
                    'waiting_time_reduction': 1.5,
                    'flow_optimization': 1.2,
                    'efficiency_improvement': 2.0
                }
            }
        }
        
        logger.info("Final RL Master AI initialized")
    
    def enable_rl_system(self, model_path: str = None):
        """Enable RL system with optional model loading"""
        self.rl_system['enabled'] = True
        self.rl_system['model_path'] = model_path
        
        if model_path and os.path.exists(model_path):
            success = self.load_rl_model(model_path)
            if success:
                logger.info(f"RL system enabled with model: {model_path}")
            else:
                logger.warning(f"RL system enabled but model loading failed: {model_path}")
        else:
            logger.info("RL system enabled without pre-trained model")
    
    def disable_rl_system(self):
        """Disable RL system"""
        self.rl_system['enabled'] = False
        logger.info("RL system disabled")
    
    def load_rl_model(self, model_path: str) -> bool:
        """Load RL model from file"""
        try:
            # Try to load as pickle first
            if model_path.endswith('.pkl'):
                import pickle
                with open(model_path, 'rb') as f:
                    model_data = pickle.load(f)
            else:
                # Try to load as JSON
                with open(model_path, 'r') as f:
                    model_data = json.load(f)
            
            # Check if it's a valid model
            if self._validate_rl_model(model_data):
                self.rl_system['model_loaded'] = True
                self.rl_system['model_path'] = model_path
                logger.info(f"RL model loaded successfully from {model_path}")
                return True
            else:
                logger.warning(f"Invalid RL model format in {model_path}")
                return False
                
        except Exception as e:
            logger.error(f"Failed to load RL model from {model_path}: {e}")
            return False
    
    def _validate_rl_model(self, model_data: Any) -> bool:
        """Validate RL model structure"""
        if isinstance(model_data, dict):
            # Check for Q-network structure
            if 'q_network' in model_data:
                q_net = model_data['q_network']
                return 'weights' in q_net and 'bias' in q_net
            
            # Check for neural network weights
            if 'weights' in model_data:
                return isinstance(model_data['weights'], (list, np.ndarray))
            
            # Check for other RL indicators
            rl_indicators = ['state_size', 'action_size', 'epsilon', 'learning_rate']
            return any(indicator in model_data for indicator in rl_indicators)
        
        # Check for direct neural network objects
        if hasattr(model_data, 'weights') and hasattr(model_data, 'bias'):
            return True
        
        return False
    
    def get_traffic_state(self, traffic_data: Dict) -> np.ndarray:
        """Convert traffic data to state vector"""
        state = np.zeros(self.rl_system['config']['state_size'])
        
        # Extract features
        idx = 0
        
        # Queue lengths
        queue_lengths = traffic_data.get('queue_lengths', {})
        for junction_id in ['I1', 'I2', 'I3', 'I4']:
            if idx < self.rl_system['config']['state_size']:
                state[idx] = queue_lengths.get(junction_id, 0) / 100.0
                idx += 1
        
        # Waiting times
        waiting_times = traffic_data.get('waiting_times', {})
        for junction_id in ['I1', 'I2', 'I3', 'I4']:
            if idx < self.rl_system['config']['state_size']:
                state[idx] = waiting_times.get(junction_id, 0) / 60.0
                idx += 1
        
        # Phase information
        if idx < self.rl_system['config']['state_size']:
            state[idx] = traffic_data.get('current_phase', 0) / 4.0
            idx += 1
        if idx < self.rl_system['config']['state_size']:
            state[idx] = traffic_data.get('phase_duration', 0) / 120.0
            idx += 1
        
        # Time context
        current_time = datetime.now()
        if idx < self.rl_system['config']['state_size']:
            state[idx] = current_time.hour / 24.0
            idx += 1
        if idx < self.rl_system['config']['state_size']:
            state[idx] = current_time.weekday() / 7.0
            idx += 1
        
        # Traffic flow
        flow_rates = traffic_data.get('flow_rates', {})
        for direction in ['north', 'south', 'east', 'west']:
            if idx < self.rl_system['config']['state_size']:
                state[idx] = flow_rates.get(direction, 0) / 1000.0
                idx += 1
        
        # Vehicle counts
        vehicle_counts = traffic_data.get('vehicle_counts', {})
        for direction in ['north', 'south', 'east', 'west']:
            if idx < self.rl_system['config']['state_size']:
                state[idx] = vehicle_counts.get(direction, 0) / 50.0
                idx += 1
        
        # Efficiency scores
        efficiency_scores = traffic_data.get('efficiency_scores', {})
        for metric in ['throughput', 'waiting_time', 'speed']:
            if idx < self.rl_system['config']['state_size']:
                state[idx] = efficiency_scores.get(metric, 0) / 100.0
                idx += 1
        
        return state
    
    def rl_action_selection(self, traffic_data: Dict) -> int:
        """RL-enhanced action selection"""
        if not self.rl_system['enabled']:
            return self._rule_based_action_selection(traffic_data)
        
        # Get state representation
        state = self.get_traffic_state(traffic_data)
        
        # RL action selection
        action = self._select_rl_action(state, traffic_data)
        
        # Record decision
        self._record_decision(action, traffic_data)
        
        return action
    
    def _select_rl_action(self, state: np.ndarray, traffic_data: Dict) -> int:
        """Select action using RL approach"""
        # Calculate Q-values for each action
        q_values = np.zeros(self.rl_system['config']['action_size'])
        
        for action in range(self.rl_system['config']['action_size']):
            expected_reward = self._calculate_action_reward(traffic_data, action)
            q_values[action] = expected_reward
        
        # Epsilon-greedy selection
        if np.random.random() < self.rl_system['config']['exploration_rate']:
            # Exploration
            action = np.random.randint(0, self.rl_system['config']['action_size'])
        else:
            # Exploitation
            action = np.argmax(q_values)
        
        return action
    
    def _calculate_action_reward(self, traffic_data: Dict, action: int) -> float:
        """Calculate expected reward for an action"""
        reward = 0
        
        # Base reward from traffic conditions
        queue_lengths = traffic_data.get('queue_lengths', {})
        avg_queue = np.mean(list(queue_lengths.values())) if queue_lengths else 0
        reward += self.rl_system['config']['reward_weights']['queue_reduction'] * max(0, 10 - avg_queue)
        
        waiting_times = traffic_data.get('waiting_times', {})
        avg_wait = np.mean(list(waiting_times.values())) if waiting_times else 0
        reward += self.rl_system['config']['reward_weights']['waiting_time_reduction'] * max(0, 20 - avg_wait)
        
        flow_rates = traffic_data.get('flow_rates', {})
        avg_flow = np.mean(list(flow_rates.values())) if flow_rates else 0
        reward += self.rl_system['config']['reward_weights']['flow_optimization'] * (avg_flow / 100)
        
        # Action-specific rewards
        action_rewards = {
            0: 5,   # Change phase
            1: 3,   # Extend green
            2: 4,   # Reduce cycle
            3: 8,   # Coordinate
            4: 15,  # Emergency
            5: 6,   # Adaptive
            6: 7,   # Queue management
            7: 9    # Flow optimization
        }
        
        reward += action_rewards.get(action, 0)
        
        return reward
    
    def _rule_based_action_selection(self, traffic_data: Dict) -> int:
        """Rule-based action selection (fallback)"""
        queue_lengths = traffic_data.get('queue_lengths', {})
        avg_queue = np.mean(list(queue_lengths.values())) if queue_lengths else 0
        
        if avg_queue > 20:
            return 6  # Queue management
        elif avg_queue > 10:
            return 1  # Extend green
        else:
            return 0  # Change phase
    
    def _record_decision(self, action: int, traffic_data: Dict):
        """Record decision for performance tracking"""
        self.rl_system['performance_tracker']['total_decisions'] += 1
        self.rl_system['performance_tracker']['action_history'].append(action)
        
        # Calculate performance score
        performance_score = self._calculate_action_reward(traffic_data, action)
        self.rl_system['performance_tracker']['performance_scores'].append(performance_score)
        
        # Keep only recent history
        if len(self.rl_system['performance_tracker']['action_history']) > 1000:
            self.rl_system['performance_tracker']['action_history'] = self.rl_system['performance_tracker']['action_history'][-500:]
            self.rl_system['performance_tracker']['performance_scores'] = self.rl_system['performance_tracker']['performance_scores'][-500:]
    
    def get_rl_performance(self) -> Dict:
        """Get RL performance metrics"""
        tracker = self.rl_system['performance_tracker']
        
        if not tracker['action_history']:
            return {
                'rl_enabled': self.rl_system['enabled'],
                'model_loaded': self.rl_system['model_loaded'],
                'total_decisions': 0
            }
        
        recent_scores = tracker['performance_scores'][-100:]
        recent_actions = tracker['action_history'][-100:]
        
        return {
            'rl_enabled': self.rl_system['enabled'],
            'model_loaded': self.rl_system['model_loaded'],
            'total_decisions': tracker['total_decisions'],
            'average_performance': np.mean(recent_scores) if recent_scores else 0,
            'performance_std': np.std(recent_scores) if recent_scores else 0,
            'action_diversity': len(set(recent_actions)),
            'most_common_action': max(set(recent_actions), key=recent_actions.count) if recent_actions else 0,
            'exploration_rate': self.rl_system['config']['exploration_rate']
        }
    
    def train_rl_system(self, training_data: List[Dict], episodes: int = 100):
        """Train RL system on provided data"""
        if not self.rl_system['enabled']:
            logger.warning("RL system not enabled")
            return False
        
        logger.info(f"Training RL system on {len(training_data)} samples for {episodes} episodes")
        
        for episode in range(episodes):
            # Sample training data
            episode_data = training_data[episode % len(training_data)]
            
            # Get state and action
            state = self.get_traffic_state(episode_data)
            action = self.rl_action_selection(episode_data)
            
            # Calculate reward
            reward = self._calculate_action_reward(episode_data, action)
            
            # Update learning progress
            self.rl_system['performance_tracker']['learning_episodes'] += 1
            
            if episode % 20 == 0:
                logger.info(f"Episode {episode}: Action {action}, Reward {reward:.2f}")
        
        logger.info("RL system training completed")
        return True
    
    def save_rl_system(self, filepath: str = None):
        """Save RL system state"""
        if filepath is None:
            filepath = f"models/final_rl_master_ai_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        system_data = {
            'rl_system': self.rl_system,
            'base_config': self.config,
            'timestamp': datetime.now().isoformat()
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w') as f:
            json.dump(system_data, f, indent=2, default=str)
        
        logger.info(f"RL system saved to {filepath}")
    
    def load_rl_system(self, filepath: str) -> bool:
        """Load RL system state"""
        try:
            with open(filepath, 'r') as f:
                system_data = json.load(f)
            
            self.rl_system = system_data.get('rl_system', self.rl_system)
            
            logger.info(f"RL system loaded from {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load RL system: {e}")
            return False

def create_final_rl_master_ai(model_path: str = None) -> FinalRLMasterAI:
    """Create final RL Master AI instance"""
    config = {
        'learning_rate': 0.001,
        'discount_factor': 0.95,
        'epsilon': 0.1,
        'epsilon_decay': 0.995,
        'epsilon_min': 0.01,
        'memory_size': 10000,
        'batch_size': 32,
        'target_update_frequency': 100,
        'training_frequency': 4,
        'video_path': "Traffic_videos/stock-footage-drone-shot-way-intersection.webm",
        'model_save_path': "models/master_ai_model.pkl",
        'experience_save_path': "data/experience_buffer.pkl"
    }
    
    rl_ai = FinalRLMasterAI(config)
    
    if model_path:
        rl_ai.enable_rl_system(model_path)
    else:
        rl_ai.enable_rl_system()
    
    return rl_ai

def main():
    """Main function for testing final RL Master AI"""
    print("üöÄ Final RL Integration Solution")
    print("=" * 50)
    
    # Create final RL Master AI
    rl_ai = create_final_rl_master_ai()
    
    # Test with sample traffic data
    test_traffic_data = {
        'queue_lengths': {'I1': 5, 'I2': 10, 'I3': 3, 'I4': 8},
        'waiting_times': {'I1': 15, 'I2': 25, 'I3': 10, 'I4': 20},
        'vehicle_counts': {'north': 12, 'south': 8, 'east': 15, 'west': 10},
        'flow_rates': {'north': 200, 'south': 150, 'east': 180, 'west': 160},
        'current_phase': 1,
        'phase_duration': 30,
        'efficiency_scores': {'throughput': 85, 'waiting_time': 70, 'speed': 90}
    }
    
    print("üß™ Testing Final RL Master AI...")
    
    # Test action selection
    actions = []
    for i in range(10):
        action = rl_ai.rl_action_selection(test_traffic_data)
        actions.append(action)
        print(f"   Decision {i+1}: Action {action}")
    
    # Get performance metrics
    performance = rl_ai.get_rl_performance()
    print(f"\nüìä RL Performance Metrics:")
    print(f"   RL Enabled: {performance['rl_enabled']}")
    print(f"   Model Loaded: {performance['model_loaded']}")
    print(f"   Total Decisions: {performance['total_decisions']}")
    print(f"   Average Performance: {performance['average_performance']:.2f}")
    print(f"   Action Diversity: {performance['action_diversity']}")
    print(f"   Most Common Action: {performance['most_common_action']}")
    
    # Save system
    rl_ai.save_rl_system()
    
    print(f"\n‚úÖ Final RL Master AI created and tested successfully!")
    print(f"üéØ Your Master AI now has integrated RL capabilities!")
    print(f"üìÅ System saved for future use")

if __name__ == "__main__":
    main()
