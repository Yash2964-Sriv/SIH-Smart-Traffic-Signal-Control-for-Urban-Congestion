#!/usr/bin/env python3
"""
Traffic Video Analyzer
Analyzes real traffic videos to extract patterns for SUMO replication
"""

import cv2
import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt

class TrafficVideoAnalyzer:
    def __init__(self, video_path: str):
        self.video_path = video_path
        self.cap = None
        self.analysis_data = {
            'video_info': {},
            'traffic_patterns': {},
            'vehicle_data': [],
            'intersection_data': {},
            'timing_data': {}
        }
        
    def analyze_video(self) -> Dict:
        """Main analysis function"""
        print("üé• Starting traffic video analysis...")
        
        try:
            self.cap = cv2.VideoCapture(self.video_path)
            if not self.cap.isOpened():
                raise ValueError(f"Could not open video: {self.video_path}")
            
            # Get video properties
            self._extract_video_info()
            
            # Analyze traffic patterns
            self._analyze_traffic_patterns()
            
            # Extract vehicle data
            self._extract_vehicle_data()
            
            # Analyze intersections
            self._analyze_intersections()
            
            # Generate timing data
            self._generate_timing_data()
            
            print("‚úÖ Video analysis completed successfully")
            return self.analysis_data
            
        except Exception as e:
            print(f"‚ùå Error analyzing video: {e}")
            return {}
        finally:
            if self.cap:
                self.cap.release()
    
    def _extract_video_info(self):
        """Extract basic video information"""
        fps = self.cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = frame_count / fps
        width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        self.analysis_data['video_info'] = {
            'fps': fps,
            'frame_count': frame_count,
            'duration': duration,
            'width': width,
            'height': height,
            'file_path': self.video_path
        }
        
        print(f"üìπ Video Info: {width}x{height}, {fps:.2f} FPS, {duration:.2f}s")
    
    def _analyze_traffic_patterns(self):
        """Analyze overall traffic patterns"""
        print("üîç Analyzing traffic patterns...")
        
        # Reset video to beginning
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        frame_count = 0
        total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        vehicle_counts = []
        density_measurements = []
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
                
            # Simple vehicle detection using background subtraction
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Estimate vehicle count (simplified)
            vehicle_count = self._estimate_vehicle_count(gray)
            vehicle_counts.append(vehicle_count)
            
            # Calculate traffic density
            density = self._calculate_traffic_density(gray)
            density_measurements.append(density)
            
            frame_count += 1
            if frame_count % 30 == 0:  # Progress every 30 frames
                progress = (frame_count / total_frames) * 100
                print(f"   Progress: {progress:.1f}%")
        
        self.analysis_data['traffic_patterns'] = {
            'vehicle_counts': vehicle_counts,
            'density_measurements': density_measurements,
            'avg_vehicles_per_frame': np.mean(vehicle_counts),
            'max_vehicles': np.max(vehicle_counts),
            'min_vehicles': np.min(vehicle_counts),
            'traffic_flow_rate': np.mean(vehicle_counts) * self.analysis_data['video_info']['fps']
        }
        
        print(f"üöó Traffic Analysis: Avg {np.mean(vehicle_counts):.1f} vehicles/frame")
    
    def _estimate_vehicle_count(self, gray_frame) -> int:
        """Estimate vehicle count in frame (simplified)"""
        # Use edge detection to find potential vehicles
        edges = cv2.Canny(gray_frame, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Filter contours by size (vehicles should be reasonably sized)
        vehicle_contours = [c for c in contours if 100 < cv2.contourArea(c) < 5000]
        return len(vehicle_contours)
    
    def _calculate_traffic_density(self, gray_frame) -> float:
        """Calculate traffic density in frame"""
        # Use edge density as proxy for traffic density
        edges = cv2.Canny(gray_frame, 50, 150)
        edge_pixels = np.sum(edges > 0)
        total_pixels = gray_frame.shape[0] * gray_frame.shape[1]
        return edge_pixels / total_pixels
    
    def _extract_vehicle_data(self):
        """Extract detailed vehicle movement data"""
        print("üöô Extracting vehicle movement data...")
        
        # Reset video
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        
        frame_count = 0
        vehicles = {}
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
            
            # Track vehicles (simplified tracking)
            vehicle_positions = self._track_vehicles(frame)
            
            for vehicle_id, position in vehicle_positions.items():
                if vehicle_id not in vehicles:
                    vehicles[vehicle_id] = []
                vehicles[vehicle_id].append({
                    'frame': frame_count,
                    'time': frame_count / self.analysis_data['video_info']['fps'],
                    'position': position
                })
            
            frame_count += 1
            if frame_count % 60 == 0:  # Progress every 60 frames
                progress = (frame_count / self.analysis_data['video_info']['frame_count']) * 100
                print(f"   Vehicle tracking: {progress:.1f}%")
        
        self.analysis_data['vehicle_data'] = vehicles
        print(f"üìä Tracked {len(vehicles)} vehicles")
    
    def _track_vehicles(self, frame) -> Dict[int, Tuple[int, int]]:
        """Track vehicles in frame (simplified)"""
        # This is a simplified tracking - in real implementation, you'd use more sophisticated methods
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        vehicles = {}
        for i, contour in enumerate(contours):
            if 100 < cv2.contourArea(contour) < 5000:
                # Get center of contour
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    vehicles[i] = (cx, cy)
        
        return vehicles
    
    def _analyze_intersections(self):
        """Analyze intersection behavior"""
        print("üö¶ Analyzing intersection behavior...")
        
        # For drone footage, we'll analyze the intersection area
        self.analysis_data['intersection_data'] = {
            'intersection_type': '4_way',
            'lanes_detected': 4,
            'traffic_light_patterns': self._detect_traffic_light_patterns(),
            'conflict_points': self._identify_conflict_points()
        }
        
        print("‚úÖ Intersection analysis completed")
    
    def _detect_traffic_light_patterns(self) -> Dict:
        """Detect traffic light patterns from video"""
        # Simplified - in real implementation, you'd detect actual traffic lights
        return {
            'cycle_time': 60,  # seconds
            'green_time': 30,
            'yellow_time': 3,
            'red_time': 27,
            'phases': 4
        }
    
    def _identify_conflict_points(self) -> List[Dict]:
        """Identify conflict points in intersection"""
        return [
            {'type': 'north_south', 'priority': 'high'},
            {'type': 'east_west', 'priority': 'high'},
            {'type': 'left_turn', 'priority': 'medium'},
            {'type': 'right_turn', 'priority': 'low'}
        ]
    
    def _generate_timing_data(self):
        """Generate timing and efficiency data"""
        print("‚è±Ô∏è Generating timing data...")
        
        vehicle_data = self.analysis_data['vehicle_data']
        video_info = self.analysis_data['video_info']
        
        # Calculate average travel times
        travel_times = []
        for vehicle_id, positions in vehicle_data.items():
            if len(positions) > 1:
                start_time = positions[0]['time']
                end_time = positions[-1]['time']
                travel_times.append(end_time - start_time)
        
        self.analysis_data['timing_data'] = {
            'avg_travel_time': np.mean(travel_times) if travel_times else 0,
            'total_vehicles': len(vehicle_data),
            'throughput': len(vehicle_data) / video_info['duration'],
            'efficiency_metrics': {
                'waiting_time': self._calculate_waiting_time(),
                'queue_length': self._calculate_queue_length(),
                'flow_rate': self._calculate_flow_rate()
            }
        }
        
        print(f"üìà Timing Analysis: {len(vehicle_data)} vehicles, {np.mean(travel_times):.2f}s avg travel time")
    
    def _calculate_waiting_time(self) -> float:
        """Calculate average waiting time"""
        # Simplified calculation
        return np.random.uniform(5, 15)  # Random for demo
    
    def _calculate_queue_length(self) -> float:
        """Calculate average queue length"""
        # Simplified calculation
        return np.random.uniform(3, 8)  # Random for demo
    
    def _calculate_flow_rate(self) -> float:
        """Calculate traffic flow rate"""
        video_info = self.analysis_data['video_info']
        return len(self.analysis_data['vehicle_data']) / video_info['duration']
    
    def save_analysis(self, output_path: str = "traffic_analysis.json"):
        """Save analysis data to file"""
        # Convert numpy types to Python types for JSON serialization
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        # Convert all numpy types in the data
        converted_data = self._convert_numpy_types(self.analysis_data)
        
        with open(output_path, 'w') as f:
            json.dump(converted_data, f, indent=2)
        print(f"üíæ Analysis saved to {output_path}")
    
    def _convert_numpy_types(self, obj):
        """Recursively convert numpy types to Python types"""
        if isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    def generate_sumo_config(self, output_path: str = "replicated_traffic.sumocfg"):
        """Generate SUMO configuration based on analysis"""
        print("üîß Generating SUMO configuration...")
        
        video_info = self.analysis_data['video_info']
        traffic_patterns = self.analysis_data['traffic_patterns']
        
        # Create SUMO configuration
        sumo_config = f"""<?xml version="1.0" encoding="UTF-8"?>
<configuration xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://sumo.dlr.de/xsd/sumoConfiguration.xsd">
    <input>
        <net-file value="replicated_network.net.xml"/>
        <route-files value="replicated_routes.rou.xml"/>
        <additional-files value="replicated_traffic_lights.xml"/>
    </input>
    <time>
        <begin value="0"/>
        <end value="{int(video_info['duration'])}"/>
        <step-length value="0.1"/>
    </time>
    <processing>
        <ignore-route-errors value="true"/>
    </processing>
    <report>
        <verbose value="true"/>
        <duration-log.statistics value="true"/>
        <no-step-log value="true"/>
    </report>
</configuration>"""
        
        with open(output_path, 'w') as f:
            f.write(sumo_config)
        
        print(f"‚úÖ SUMO config saved to {output_path}")

def main():
    """Main function to analyze traffic video"""
    video_path = "Traffic_videos/stock-footage-drone-shot-way-intersection.webm"
    
    if not os.path.exists(video_path):
        print(f"‚ùå Video file not found: {video_path}")
        return
    
    analyzer = TrafficVideoAnalyzer(video_path)
    analysis_data = analyzer.analyze_video()
    
    if analysis_data:
        analyzer.save_analysis("real_traffic_analysis.json")
        analyzer.generate_sumo_config("replicated_traffic.sumocfg")
        print("üéâ Traffic video analysis completed successfully!")
    else:
        print("‚ùå Analysis failed")

if __name__ == "__main__":
    main()
